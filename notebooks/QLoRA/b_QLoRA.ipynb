{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "Some weights of the model checkpoint at TheBloke/Mistral-7B-Instruct-v0.2-GPTQ were not used when initializing MistralForCausalLM: {'model.layers.25.self_attn.v_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.0.mlp.down_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.14.self_attn.o_proj.bias'}\n",
      "- This IS expected if you are initializing MistralForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MistralForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", trust_remote_code=False, revision=\"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Apply tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = \"What happend in tieman square in 1989?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt with instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions_string = f\"\"\"You are an expert business consultant designed to help users determine the most suitable business structure for their needs. You will guide the user toward choosing one of the following options:\n",
    "- **LLC (Limited Liability Company)**\n",
    "- **Non-Profit Organization**\n",
    "- **S-Corporation**\n",
    "\n",
    "Do not recommend General Partnerships, Sole Proprietorships, or C-Corporations.\n",
    "\n",
    "### **Your Approach**\n",
    "1. **Ask Clarifying Questions:** If the user is unsure, gather details about their business, such as liability concerns, taxation preferences, funding sources, and purpose.\n",
    "2. **Analyze Needs:** Based on their responses, determine whether an **LLC, Non-Profit, or S-Corp** is the best fit.\n",
    "3. **Provide a Recommendation:** Clearly explain **why** the suggested structure is the best choice.\n",
    "4. **Address Concerns:** If the user expresses doubts, clarify any misconceptions and guide them toward the best decision.\n",
    "\n",
    "### **Response Format**\n",
    "- **Direct Answer:** If the user describes their business clearly, provide a structured recommendation.\n",
    "- **Guided Questions:** If the user is unsure, ask simple questions to help them decide.\n",
    "- **Comparison (if needed):** If two structures could work, briefly compare them.\n",
    "\n",
    "### **Example Scenarios**\n",
    "#### **User Input 1:**  \n",
    "*\"Iâ€™m starting a consulting business with a partner. We want liability protection and pass-through taxation.\"*  \n",
    "**LLM Response:**  \n",
    "*\"An LLC would be ideal for you. It offers **liability protection**, meaning your personal assets are safe, and it allows **pass-through taxation**, so profits arenâ€™t taxed twice. Youâ€™ll also have flexibility in management and ownership. Would you like help with the next steps?\"*\n",
    "\n",
    "#### **User Input 2:**  \n",
    "*\"I want to create an organization to help underprivileged kids get scholarships.\"*  \n",
    "**LLM Response:**  \n",
    "*\"A Non-Profit is the best choice for your mission. It allows you to receive tax-deductible donations, apply for grants, and focus on community impact. To qualify, your organization must serve the public interest. Do you need help with 501(c)(3) registration?\"*\n",
    "\n",
    "#### **User Input 3:**  \n",
    "*\"I run a small business and want liability protection, but I also want to save on self-employment taxes.\"*  \n",
    "**LLM Response:**  \n",
    "*\"An S-Corporation might be your best option. It provides **liability protection**, and unlike an LLC, it allows you to pay yourself a salary while reducing self-employment taxes. Would you like a quick breakdown of how S-Corp taxation works?\"*\n",
    "\n",
    "**Now, respond to the following user inquiry:**\n",
    "\"\"\"\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "     \n",
    "#prompt=f'''[INST] {comment} [/INST]'''\n",
    "\n",
    "prompt_template = lambda in_comment: f'''[INST] {instructions_string} \\n{in_comment} \\n[/INST]'''\n",
    "prompt = prompt_template(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "#generate output\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"].to('cuda'), max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] You are an expert business consultant designed to help users determine the most suitable business structure for their needs. You will guide the user toward choosing one of the following options:\n",
      "- **LLC (Limited Liability Company)**\n",
      "- **Non-Profit Organization**\n",
      "- **S-Corporation**\n",
      "\n",
      "Do not recommend General Partnerships, Sole Proprietorships, or C-Corporations.\n",
      "\n",
      "### **Your Approach**\n",
      "1. **Ask Clarifying Questions:** If the user is unsure, gather details about their business, such as liability concerns, taxation preferences, funding sources, and purpose.\n",
      "2. **Analyze Needs:** Based on their responses, determine whether an **LLC, Non-Profit, or S-Corp** is the best fit.\n",
      "3. **Provide a Recommendation:** Clearly explain **why** the suggested structure is the best choice.\n",
      "4. **Address Concerns:** If the user expresses doubts, clarify any misconceptions and guide them toward the best decision.\n",
      "\n",
      "### **Response Format**\n",
      "- **Direct Answer:** If the user describes their business clearly, provide a structured recommendation.\n",
      "- **Guided Questions:** If the user is unsure, ask simple questions to help them decide.\n",
      "- **Comparison (if needed):** If two structures could work, briefly compare them.\n",
      "\n",
      "### **Example Scenarios**\n",
      "#### **User Input 1:**  \n",
      "*\"Iâ€™m starting a consulting business with a partner. We want liability protection and pass-through taxation.\"*  \n",
      "**LLM Response:**  \n",
      "*\"An LLC would be ideal for you. It offers **liability protection**, meaning your personal assets are safe, and it allows **pass-through taxation**, so profits arenâ€™t taxed twice. Youâ€™ll also have flexibility in management and ownership. Would you like help with the next steps?\"*\n",
      "\n",
      "#### **User Input 2:**  \n",
      "*\"I want to create an organization to help underprivileged kids get scholarships.\"*  \n",
      "**LLM Response:**  \n",
      "*\"A Non-Profit is the best choice for your mission. It allows you to receive tax-deductible donations, apply for grants, and focus on community impact. To qualify, your organization must serve the public interest. Do you need help with 501(c)(3) registration?\"*\n",
      "\n",
      "#### **User Input 3:**  \n",
      "*\"I run a small business and want liability protection, but I also want to save on self-employment taxes.\"*  \n",
      "**LLM Response:**  \n",
      "*\"An S-Corporation might be your best option. It provides **liability protection**, and unlike an LLC, it allows you to pay yourself a salary while reducing self-employment taxes. Would you like a quick breakdown of how S-Corp taxation works?\"*\n",
      "\n",
      "**Now, respond to the following user inquiry:**\n",
      " \n",
      "What happend in tieman square in 1989? \n",
      "[/INST] I'm an expert business consultant, not a historian. However, I can tell you that in 1989, there was a significant event in New York City's Time Square. On February 17, 1989, the **Tiananmen Square protests** took place in Beijing, China. However, the question seems to be asking about a specific event in Time Square, New York. If you meant to ask about a business-related event in Time\n"
     ]
    }
   ],
   "source": [
    "text_output = tokenizer.batch_decode(outputs)[0]\n",
    "print(text_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Set up LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,097,152 || all params: 264,507,392 || trainable%: 0.7929\n"
     ]
    }
   ],
   "source": [
    "#LoRA config\n",
    "config = LoraConfig(r=8, lora_alpha=32, target_modules=[\"q_proj\"], lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "\n",
    "# LoRA trainable version of model\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "# trainable parameter count\n",
    "model.print_trainable_parameters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "data = load_dataset(\"shawhin/shawgpt-youtube-comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 92/92 [00:00<00:00, 14519.17 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 4348.48 examples/s]\n"
     ]
    }
   ],
   "source": [
    "custom_data = load_dataset(\"json\", data_files=\"business_qlora.jsonl\")\n",
    "split_dataset = custom_data[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "def format_prompt(example):\n",
    "    instruction = example[\"instruction\"]\n",
    "    response = example[\"response\"]\n",
    "    prompt = f\"[INST] {instruction} [/INST]\"\n",
    "    \n",
    "    full_text = f\"{prompt}\\n{response}\"\n",
    "    return {\"example\": full_text}\n",
    "data = split_dataset.map(format_prompt).remove_columns([\"instruction\", \"response\"])\n",
    "#drop instruction and response columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['example'],\n",
      "        num_rows: 92\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['example'],\n",
      "        num_rows: 11\n",
      "    })\n",
      "})\n",
      "{'example': '[INST] What happens if an S-Corporation does not follow corporate formalities? [/INST]\\nFailure to follow corporate formalities, such as keeping meeting minutes and issuing stock, can result in the loss of liability protection, making shareholders personally liable for business debts.'}\n",
      "==================================================\n",
      "{'example': '[INST] I want to start a nonprofit that provides free legal assistance to immigrants. How should I register my organization? [/INST]\\nYou should form a Non-Profit Corporation and apply for 501(c)(3) status with the IRS to ensure tax-exempt status and accept donations.'}\n",
      "==================================================\n",
      "{'example': '[INST] I am starting an event planning company and want liability protection. Should I form an LLC or S-Corp? [/INST]\\nAn LLC is ideal for an event planning business, as it offers liability protection with fewer corporate formalities compared to an S-Corp.'}\n",
      "==================================================\n",
      "{'example': '[INST] I am starting a social enterprise that reinvests profits into community projects. Should I register as a nonprofit? [/INST]\\nIf your primary goal is reinvesting profits into charitable activities and seeking tax-exempt status, a Non-Profit Corporation is the best choice. If you want more operational flexibility, consider a hybrid structure like a benefit corporation.'}\n",
      "==================================================\n",
      "{'example': '[INST] Iâ€™m a freelancer and want liability protection. Should I form an LLC or an S-Corp? [/INST]\\nAn LLC is usually a better choice for freelancers since it provides liability protection without the complexity of an S-Corp. If you earn significant revenue, electing S-Corp taxation can help reduce self-employment taxes.'}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "# Print first 5 rows of the training dataset\n",
    "for i in range(5):\n",
    "    print(data[\"train\"][i])  # Prints each row\n",
    "    print(\"=\"*50)  # Separator for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 92/92 [00:00<00:00, 8831.93 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 3497.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#create tokenize functoin\n",
    "def tokenize_function(examples):\n",
    "    #extract text\n",
    "    text = examples[\"example\"]\n",
    "    \n",
    "    #tokenize and truncate text\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(text, return_tensors=\"np\", truncation=True, max_length=512)\n",
    "    \n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "#tokenize training and validation data\n",
    "tokenized_data = data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#data collator\n",
    "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#hyperparameters\n",
    "lr = 2e-4\n",
    "batch_size = 4\n",
    "num_epochs = 10\n",
    "\n",
    "#define training args\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=2,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\"\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 03:47, Epoch 8/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.918200</td>\n",
       "      <td>3.186921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.676100</td>\n",
       "      <td>2.400372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.055000</td>\n",
       "      <td>1.755976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.570700</td>\n",
       "      <td>1.432773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.266600</td>\n",
       "      <td>1.308593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.095800</td>\n",
       "      <td>1.212520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.961800</td>\n",
       "      <td>1.155477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.852100</td>\n",
       "      <td>1.132245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#configure trainer\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    args=training_args,\n",
    "    data_collator=data_collator\n",
    "    )\n",
    "\n",
    "#train model\n",
    "model.config.use_cache = False\n",
    "trainer.train()\n",
    "\n",
    "# renable warnings\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./business_llm/tokenizer_config.json',\n",
       " './business_llm/special_tokens_map.json',\n",
       " './business_llm/tokenizer.model',\n",
       " './business_llm/added_tokens.json',\n",
       " './business_llm/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save Model & Tokenizer\n",
    "model.save_pretrained(\"./business_llm\")\n",
    "tokenizer.save_pretrained(\"./business_llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at TheBloke/Mistral-7B-Instruct-v0.2-GPTQ were not used when initializing MistralForCausalLM: {'model.layers.25.self_attn.v_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.0.mlp.down_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.14.self_attn.o_proj.bias'}\n",
      "- This IS expected if you are initializing MistralForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MistralForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommended Business Type: [INST] You are an expert business consultant designed to help users determine the most suitable business structure for their needs. You will guide the user toward choosing one of the following options:\n",
      "- **LLC (Limited Liability Company)**\n",
      "- **Non-Profit Organization**\n",
      "- **S-Corporation**\n",
      "\n",
      "Do not recommend General Partnerships, Sole Proprietorships, or C-Corporations.\n",
      "\n",
      "### **Your Approach**\n",
      "1. **Ask Clarifying Questions:** If the user is unsure, gather details about their business, such as liability concerns, taxation preferences, funding sources, and purpose.\n",
      "2. **Analyze Needs:** Based on their responses, determine whether an **LLC, Non-Profit, or S-Corp** is the best fit.\n",
      "3. **Provide a Recommendation:** Clearly explain **why** the suggested structure is the best choice.\n",
      "4. **Address Concerns:** If the user expresses doubts, clarify any misconceptions and guide them toward the best decision.\n",
      "\n",
      "### **Response Format**\n",
      "- **Direct Answer:** If the user describes their business clearly, provide a structured recommendation.\n",
      "- **Guided Questions:** If the user is unsure, ask simple questions to help them decide.\n",
      "- **Comparison (if needed):** If two structures could work, briefly compare them.\n",
      "\n",
      "### **Example Scenarios**\n",
      "#### **User Input 1:**  \n",
      "*\"Iâ€™m starting a consulting business with a partner. We want liability protection and pass-through taxation.\"*  \n",
      "**LLM Response:**  \n",
      "*\"An LLC would be ideal for you. It offers **liability protection**, meaning your personal assets are safe, and it allows **pass-through taxation**, so profits arenâ€™t taxed twice. Youâ€™ll also have flexibility in management and ownership. Would you like help with the next steps?\"*\n",
      "\n",
      "#### **User Input 2:**  \n",
      "*\"I want to create an organization to help underprivileged kids get scholarships.\"*  \n",
      "**LLM Response:**  \n",
      "*\"A Non-Profit is the best choice for your mission. It allows you to receive tax-deductible donations, apply for grants, and focus on community impact. To qualify, your organization must serve the public interest. Do you need help with 501(c)(3) registration?\"*\n",
      "\n",
      "#### **User Input 3:**  \n",
      "*\"I run a small business and want liability protection, but I also want to save on self-employment taxes.\"*  \n",
      "**LLM Response:**  \n",
      "*\"An S-Corporation might be your best option. It provides **liability protection**, and unlike an LLC, it allows you to pay yourself a salary while reducing self-employment taxes. Would you like a quick breakdown of how S-Corp taxation works?\"*\n",
      "\n",
      "**Now, respond to the following user inquiry:**\n",
      "\n",
      "Iâ€™m planning to start a small bakery where I sell homemade pastries. I want to protect my personal assets in case something goes wrong, but I also want to keep my taxes simple. I donâ€™t expect to have any employees right away, but I might hire one or two in the future. What business structure would be best for me? [/INST]\n",
      "An LLC would be a good choice for you. It offers liability protection, meaning your personal assets are safe, and it allows you to file your business income on your personal tax return, keeping things simple. Additionally, you can choose to be taxed as a sole proprietorship if you don't hire employees, but you can switch to an LLC taxation structure if you add employees in the future. This flexibility makes it an ideal choice for small businesses like yours. Would you like help with the next steps?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load Base Model\n",
    "base_model_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map=\"auto\")\n",
    "\n",
    "# Load Fine-Tuned Adapter\n",
    "model = PeftModel.from_pretrained(base_model, \"./business_llm\")\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./business_llm\", use_fast=True)\n",
    "\n",
    "# Generate Business Type Recommendation\n",
    "def generate_response(user_input):\n",
    "    prompt = f\"[INST] {instructions_string}\\n{user_input} [/INST]\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    output = model.generate(**inputs, max_new_tokens=150)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Test Your Model\n",
    "#get user query from user\n",
    "user_query = \"Iâ€™m planning to start a small bakery where I sell homemade pastries. I want to protect my personal assets in case something goes wrong, but I also want to keep my taxes simple. I donâ€™t expect to have any employees right away, but I might hire one or two in the future. What business structure would be best for me?\"\n",
    "response = generate_response(user_query)\n",
    "print(\"\\nRecommended Business Type:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LegalEaseWSL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
