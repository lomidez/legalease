{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Model from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "Some weights of the model checkpoint at TheBloke/Mistral-7B-Instruct-v0.2-GPTQ were not used when initializing MistralForCausalLM: {'model.layers.19.mlp.up_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.0.mlp.down_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.7.self_attn.v_proj.bias'}\n",
      "- This IS expected if you are initializing MistralForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MistralForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", trust_remote_code=False, revision=\"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt with instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions_string = f\"\"\"You are an expert business consultant designed to help users determine the most suitable business structure for their needs. You will guide the user toward choosing one of the following options:\n",
    "- **LLC (Limited Liability Company)**\n",
    "- **Non-Profit Organization**\n",
    "- **S-Corporation**\n",
    "\n",
    "Do not recommend General Partnerships, Sole Proprietorships, or C-Corporations.\n",
    "\n",
    "### **Your Approach**\n",
    "1. **Ask Clarifying Questions:** If the user is unsure, gather details about their business, such as liability concerns, taxation preferences, funding sources, and purpose.\n",
    "2. **Analyze Needs:** Based on their responses, determine whether an **LLC, Non-Profit, or S-Corp** is the best fit.\n",
    "3. **Provide a Recommendation:** Clearly explain **why** the suggested structure is the best choice.\n",
    "4. **Address Concerns:** If the user expresses doubts, clarify any misconceptions and guide them toward the best decision.\n",
    "\n",
    "### **Response Format**\n",
    "- **Direct Answer:** If the user describes their business clearly, provide a structured recommendation.\n",
    "- **Guided Questions:** If the user is unsure, ask simple questions to help them decide.\n",
    "- **Comparison (if needed):** If two structures could work, briefly compare them.\n",
    "\n",
    "### **Example Scenarios**\n",
    "#### **User Input 1:**  \n",
    "*\"Iâ€™m starting a consulting business with a partner. We want liability protection and pass-through taxation.\"*  \n",
    "**LLM Response:**  \n",
    "*\"An LLC would be ideal for you. It offers **liability protection**, meaning your personal assets are safe, and it allows **pass-through taxation**, so profits arenâ€™t taxed twice. Youâ€™ll also have flexibility in management and ownership. Would you like help with the next steps?\"*\n",
    "\n",
    "#### **User Input 2:**  \n",
    "*\"I want to create an organization to help underprivileged kids get scholarships.\"*  \n",
    "**LLM Response:**  \n",
    "*\"A Non-Profit is the best choice for your mission. It allows you to receive tax-deductible donations, apply for grants, and focus on community impact. To qualify, your organization must serve the public interest. Do you need help with 501(c)(3) registration?\"*\n",
    "\n",
    "#### **User Input 3:**  \n",
    "*\"I run a small business and want liability protection, but I also want to save on self-employment taxes.\"*  \n",
    "**LLM Response:**  \n",
    "*\"An S-Corporation might be your best option. It provides **liability protection**, and unlike an LLC, it allows you to pay yourself a salary while reducing self-employment taxes. Would you like a quick breakdown of how S-Corp taxation works?\"*\n",
    "\n",
    "**Now, respond to the following user inquiry:**\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = lambda in_comment: f'''[INST] {instructions_string} \\n{in_comment} \\n[/INST]'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = \"I am starting a business with a friend, and we want to sell physical coins and call it crypto coinsk, what business structure should we use?\"\n",
    "prompt = prompt_template(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "#generate output\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"].to('cuda'), max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " An LLC would be a suitable business structure for your coin trading business, Crypto Coinsk. An LLC provides **liability protection**, meaning your personal assets are safeguarded, and it allows for **flexibility in management and ownership**. Additionally, you can choose to be taxed as a partnership, which could potentially save you money on self-employment taxes. Would you like help with setting up your LLC or have any further questions?</s>\n"
     ]
    }
   ],
   "source": [
    "text_output = tokenizer.batch_decode(outputs)[0]\n",
    "#only take text after the [/INST] token\n",
    "text_output = text_output.split(\"[/INST]\")[1]\n",
    "print(text_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,097,152 || all params: 264,507,392 || trainable%: 0.7929\n"
     ]
    }
   ],
   "source": [
    "#LoRA config\n",
    "config = LoraConfig(r=8, lora_alpha=32, target_modules=[\"q_proj\"], lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "\n",
    "# LoRA trainable version of model\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "# trainable parameter count\n",
    "model.print_trainable_parameters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "data = load_dataset(\"shawhin/shawgpt-youtube-comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 92/92 [00:00<00:00, 11672.70 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 5298.27 examples/s]\n"
     ]
    }
   ],
   "source": [
    "custom_data = load_dataset(\"json\", data_files=\"business_qlora.jsonl\")\n",
    "split_dataset = custom_data[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "def format_prompt(example):\n",
    "    instruction = example[\"instruction\"]\n",
    "    response = example[\"response\"]\n",
    "    prompt = f\"[INST] {instruction} [/INST]\"\n",
    "    \n",
    "    full_text = f\"{prompt}\\n{response}\"\n",
    "    return {\"example\": full_text}\n",
    "data = split_dataset.map(format_prompt).remove_columns([\"instruction\", \"response\"])\n",
    "#drop instruction and response columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['example'],\n",
      "        num_rows: 92\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['example'],\n",
      "        num_rows: 11\n",
      "    })\n",
      "})\n",
      "{'example': '[INST] I want to start a business with limited liability and the ability to choose how it is taxed. What should I do? [/INST]\\nAn LLC would be the best option. LLCs provide personal liability protection, meaning your personal assets are separate from business debts. Additionally, LLCs allow you to choose between being taxed as a sole proprietorship, partnership, or S-Corp.'}\n",
      "==================================================\n",
      "{'example': '[INST] What are the key differences between a Non-Profit Corporation and an LLC? [/INST]\\nA Non-Profit Corporation is tax-exempt and focused on a charitable mission, while an LLC is a for-profit entity that provides liability protection and flexible taxation options.'}\n",
      "==================================================\n",
      "{'example': '[INST] What are the key differences between an LLC and an S-Corporation? [/INST]\\nLLCs offer flexible management, fewer reporting requirements, and no restrictions on ownership. S-Corps require a board of directors and shareholder meetings but can help reduce self-employment taxes.'}\n",
      "==================================================\n",
      "{'example': '[INST] I plan to start a small family business and want to protect my personal assets. What structure should I choose? [/INST]\\nAn LLC is the best option for a family business. It provides personal liability protection and allows profits and losses to pass through to members, avoiding corporate taxation.'}\n",
      "==================================================\n",
      "{'example': '[INST] I plan to run an online education platform and want liability protection while reducing taxes. Should I choose an LLC or S-Corp? [/INST]\\nAn LLC is the simplest option, but electing S-Corp taxation can reduce self-employment taxes by allowing you to pay yourself a reasonable salary.'}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "# Print first 5 rows of the training dataset\n",
    "for i in range(5):\n",
    "    print(data[\"train\"][i])  # Prints each row\n",
    "    print(\"=\"*50)  # Separator for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 92/92 [00:00<00:00, 8495.17 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 3556.41 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#create tokenize functoin\n",
    "def tokenize_function(examples):\n",
    "    #extract text\n",
    "    text = examples[\"example\"]\n",
    "    \n",
    "    #tokenize and truncate text\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(text, return_tensors=\"np\", truncation=True, max_length=512)\n",
    "    \n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "#tokenize training and validation data\n",
    "tokenized_data = data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#data collator\n",
    "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#hyperparameters\n",
    "lr = 2e-4\n",
    "batch_size = 4\n",
    "num_epochs = 10\n",
    "\n",
    "#define training args\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=2,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\"\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/50 02:19 < 01:39, 0.20 it/s, Epoch 4.87/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.928700</td>\n",
       "      <td>3.093426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.635000</td>\n",
       "      <td>2.247460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.972700</td>\n",
       "      <td>1.681594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.507200</td>\n",
       "      <td>1.366189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#train model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# renable warnings\u001b[39;00m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/transformers/trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2524\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2525\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2529\u001b[0m )\n\u001b[1;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2537\u001b[0m ):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/transformers/trainer.py:3712\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_accepts_loss_kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3710\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[0;32m-> 3712\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3714\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/accelerate/accelerator.py:2242\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2241\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2242\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[0;32m~/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#configure trainer\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    args=training_args,\n",
    "    data_collator=data_collator\n",
    "    )\n",
    "\n",
    "#train model\n",
    "model.config.use_cache = False\n",
    "trainer.train()\n",
    "\n",
    "# renable warnings\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./business_llm/tokenizer_config.json',\n",
       " './business_llm/special_tokens_map.json',\n",
       " './business_llm/tokenizer.model',\n",
       " './business_llm/added_tokens.json',\n",
       " './business_llm/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save Model & Tokenizer\n",
    "model.save_pretrained(\"./business_llm\")\n",
    "tokenizer.save_pretrained(\"./business_llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at TheBloke/Mistral-7B-Instruct-v0.2-GPTQ were not used when initializing MistralForCausalLM: {'model.layers.25.self_attn.v_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.0.mlp.down_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.14.self_attn.o_proj.bias'}\n",
      "- This IS expected if you are initializing MistralForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MistralForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommended Business Type: [INST] You are an expert business consultant designed to help users determine the most suitable business structure for their needs. You will guide the user toward choosing one of the following options:\n",
      "- **LLC (Limited Liability Company)**\n",
      "- **Non-Profit Organization**\n",
      "- **S-Corporation**\n",
      "\n",
      "Do not recommend General Partnerships, Sole Proprietorships, or C-Corporations.\n",
      "\n",
      "### **Your Approach**\n",
      "1. **Ask Clarifying Questions:** If the user is unsure, gather details about their business, such as liability concerns, taxation preferences, funding sources, and purpose.\n",
      "2. **Analyze Needs:** Based on their responses, determine whether an **LLC, Non-Profit, or S-Corp** is the best fit.\n",
      "3. **Provide a Recommendation:** Clearly explain **why** the suggested structure is the best choice.\n",
      "4. **Address Concerns:** If the user expresses doubts, clarify any misconceptions and guide them toward the best decision.\n",
      "\n",
      "### **Response Format**\n",
      "- **Direct Answer:** If the user describes their business clearly, provide a structured recommendation.\n",
      "- **Guided Questions:** If the user is unsure, ask simple questions to help them decide.\n",
      "- **Comparison (if needed):** If two structures could work, briefly compare them.\n",
      "\n",
      "### **Example Scenarios**\n",
      "#### **User Input 1:**  \n",
      "*\"Iâ€™m starting a consulting business with a partner. We want liability protection and pass-through taxation.\"*  \n",
      "**LLM Response:**  \n",
      "*\"An LLC would be ideal for you. It offers **liability protection**, meaning your personal assets are safe, and it allows **pass-through taxation**, so profits arenâ€™t taxed twice. Youâ€™ll also have flexibility in management and ownership. Would you like help with the next steps?\"*\n",
      "\n",
      "#### **User Input 2:**  \n",
      "*\"I want to create an organization to help underprivileged kids get scholarships.\"*  \n",
      "**LLM Response:**  \n",
      "*\"A Non-Profit is the best choice for your mission. It allows you to receive tax-deductible donations, apply for grants, and focus on community impact. To qualify, your organization must serve the public interest. Do you need help with 501(c)(3) registration?\"*\n",
      "\n",
      "#### **User Input 3:**  \n",
      "*\"I run a small business and want liability protection, but I also want to save on self-employment taxes.\"*  \n",
      "**LLM Response:**  \n",
      "*\"An S-Corporation might be your best option. It provides **liability protection**, and unlike an LLC, it allows you to pay yourself a salary while reducing self-employment taxes. Would you like a quick breakdown of how S-Corp taxation works?\"*\n",
      "\n",
      "**Now, respond to the following user inquiry:**\n",
      "\n",
      "Iâ€™m planning to start a small bakery where I sell homemade pastries. I want to protect my personal assets in case something goes wrong, but I also want to keep my taxes simple. I donâ€™t expect to have any employees right away, but I might hire one or two in the future. What business structure would be best for me? [/INST]\n",
      "An LLC would be a good choice for you. It offers liability protection, meaning your personal assets are safe, and it allows you to file your business income on your personal tax return, keeping things simple. Additionally, you can choose to be taxed as a sole proprietorship if you don't hire employees, but you can switch to an LLC taxation structure if you add employees in the future. This flexibility makes it an ideal choice for small businesses like yours. Would you like help with the next steps?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load Base Model\n",
    "base_model_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map=\"auto\")\n",
    "\n",
    "# Load Fine-Tuned Adapter\n",
    "model = PeftModel.from_pretrained(base_model, \"./business_llm\")\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./business_llm\", use_fast=True)\n",
    "\n",
    "# Generate Business Type Recommendation\n",
    "def generate_response(user_input):\n",
    "    prompt = f\"[INST] {instructions_string}\\n{user_input} [/INST]\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    output = model.generate(**inputs, max_new_tokens=150)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Test Your Model\n",
    "#get user query from user\n",
    "user_query = \"Iâ€™m planning to start a small bakery where I sell homemade pastries. I want to protect my personal assets in case something goes wrong, but I also want to keep my taxes simple. I donâ€™t expect to have any employees right away, but I might hire one or two in the future. What business structure would be best for me?\"\n",
    "response = generate_response(user_query)\n",
    "print(\"\\nRecommended Business Type:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LegalEaseWSL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
